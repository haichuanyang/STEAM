Apache Hadoop is an open source framework for big data. It is based on the MapReduce programming model which Google invented and published. The MapReduce model is, at its simplest, means that one function traditionally called the Map function, runs in parallel with a massive dataset to produce intermediate results. And another function, traditionally called the reduce function, builds a final result set based on all those intermediate results. The term Hadoop is often used informally to encompass Apache Hadoop itself, and related projects such as Apache Spark, Apache Pig, and Apache Hive. Cloud Dataproc is a fast, easy, managed way to run Hadoop, Spark, Hive, and Pig on Google Cloud Platform. All you have to do is request a Hadoop cluster. It will be built for you in 90 seconds or less, on top of Compute Engine virtual machines whose number and type you control. If you need more or less processing power while your cluster is running, you can scale it up or down. You can use the default configuration for the Hadoop software in your cluster or you can customize it. And you can monitor your cluster using Stackdriver. Running on-premises, Hadoop jobs requires a capital hardware investment. Running these jobs in Cloud Dataproc, allows you to only pay for hardware resources used during the life of the cluster you create. Although the rate for pricing is based on the hour, Cloud Dataproc is built by the second. Our Cloud Dataproc clusters are build in one second clock time increments. Subject to a one minute minimum billing. So, when you're done with your cluster, you can delete it, and billing stops.



This is much more agile use of resources than on-premise hardware assets. You can also save money, by telling Cloud Dataproc to use preemptible compute engine instances for your batch processing. You have to make sure that your jobs can be restarted cleanly, if they're terminated, and you get a significant break in the cost of the instances. At the time this video was made, preemptible instances were around 80 percent cheaper. Be aware that the cost of the compute engine instances isn't the only component of the cost of a Dataproc cluster, but it's a significant one. Once your data is in a cluster, you can use Spark and Spark SQL to do data mining. And you can use MLib, which is Apache Spark's machine learning libraries to discover patterns through machine learning.

data flow

That's where Cloud Dataflow is particularly good choice. It's both a unified programming model and a managed service and it lets you develop and execute a big range of data processing patterns; extract, to transform, and load batch computation and continuous computation. You use Dataflow to build data pipelines. And the same pipelines work for both batch and streaming data. There's no need to spin up a cluster or to size instances. Cloud Dataflow fully automates the management of whatever processing resources are required. Cloud Dataflow frees you from operational tasks like resource management and performance optimization. In this example, Dataflow pipeline reads data from a big query table, the source. Processes it in a variety of ways, the transforms. And writes it's output to a cloud storage, the Sync. Some of those transforms you see here are map operations and some or reduce operations. You can build really expressive pipelines. Each step in the pipeline is elastically scaled. There is no need to launch and manage a cluster. Instead, the service provides all resources on demand. It has automated and optimized worked partitioning built-in which can dynamically re-balance lagging work that reduces the need to worry about hotkeys. That is situations where just proportionately large chunks of your input get mapped to the same cluster. People use Dataflow in a variety of use cases. As we've discussed, it's a general purpose ETL tool and its use case as a data analysis engine comes in handy in things like fraud detection and financial services, IoT analytics in manufacturing, healthcare and logistics and click stream, point of sale and segmentation analysis in retail. And because those pipelines, we saw can orchestrate multiple services even external services. It can be used in real time applications such as personalizing gaming user experiences.


bigquery

Suppose, instead of a dynamic pipeline, your data needs to run more in the way of exploring a vast sea of data. You want to do ad hoc SQL queries on a massive data set. That's what BigQuery is for. It's Google's fully manage petabyte scale, low cost analytics data warehouse. Because there's no infrastructure to manage, you can focus on analyzing data to find meaningful insights. Use familiar SQL and take advantage of our pay-as-you-go model. It's easy to get data into BigQuery. You can load it from cloud storage or cloud data store, or stream it into Big Query at up to 100,000 rows per second. Once it's in there, you can run super fast SQL queries against multiple terabytes of data in seconds using the processing power of Google's infrastructure. In addition to SQL queries, you can easily read and write data in BigQuery via Cloud Dataflow, Hadoop, and Spark. BigQuery is used by all types of organizations from startups to Fortune 500 companies, smaller organizations like Big Query's free monthly quotas, bigger organizations like its seamless scale, and it's available 99.9 percent service level agreement.
1:26
Google's infrastructure is global and so is BigQuery. BigQuery lets you specify the region where your data will be kept. So, for example, if you want to keep data in Europe, you don't have to go set-up a cluster in Europe. Just specify the EU location where you create your data set. US and Asia locations are also available. Because BigQuery separates storage and computation, you pay for your data storage separately from queries. That means, you pay for queries only when they are actually running. You have full control over who has access to the data stored in BigQuery, including sharing data sets with people in different projects. If you share data sets that won't impact your cost or performance, people you share with pay for their own queries, not you. Long term storage pricing is an automatic discount for data residing in BigQuery for extended periods of time. When the age of your data reaches 90 days in BigQuery, Google will automatically drop the price of storage.


pub/sub

datalab

messaging service


can configure your subscribers to receive messages on a push or pull basis. In other words, subscribers can get notified when new messages arrive for them or they can check for new messages at intervals. Scientists have long used lab notebooks to organize their thoughts and explore their data. For data science, the lab notebook metaphor works really well, because it feels natural to intersperse data analysis with comments about their results. A popular environment for hosting those is Project Jupyter. It lets you create and maintain web-based notebooks containing Python code and you can run that code interactively and view the results. And Cloud Datalab takes the management work out of this natural technique. It runs in a Compute Engine virtual machine. To get started, you specify the virtual machine type you want and what GCP region it should run in. When it launches, it presents an interactive Python environment that's ready to use. And it orchestrates multiple GCP services automatically, so you can focus on exploring your data. You only pay for the resources you use. There is no additional charge for Datalab itself. It's integrated with BigQuery, Compute Engine, and Cloud Storage, so accessing your data doesn't run into authentication hustles. When you're up and running, you can visualize your data with Google charts or map plot line and because there's a vibrant interactive Python community, you can learn from published notebooks. There are many existing packages for statistics, machine learning, and so on.
